{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62d5b10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda Environment: lickTask2\n",
      "python version: 3.9.10\n"
     ]
    }
   ],
   "source": [
    "# widen jupyter notebook window\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:95% !important; }</style>\"))\n",
    "\n",
    "# check environment\n",
    "import os\n",
    "print(f'Conda Environment: ' + os.environ['CONDA_DEFAULT_ENV'])\n",
    "\n",
    "from platform import python_version\n",
    "print(f'python version: {python_version()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23f9143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model 2ABT data with Q-learning algorithms.\n",
    "\"\"\"\n",
    "\n",
    "# descriptor 'bin_date': describes which binned set of days to compare to 'before' days.\n",
    "# modelID: 'standard' (contains both alpha/zeta) or 'reduced' (combines alpha/zeta into one parameter).\n",
    "params = {\n",
    "    'paths': {\n",
    "        'dir_data': 'path/to/2ABT_dataframe',\n",
    "        'filename': '2ABT_dataframe.csv',\n",
    "    },\n",
    "    'hyper_params': {\n",
    "        'split': 4,\n",
    "        'train_prop': 0.75,\n",
    "        'lr': 0.1,\n",
    "        'n_iter': 10000,\n",
    "        'seed': 42,\n",
    "    },\n",
    "    'descriptors': {\n",
    "        'geno': 'genotype',\n",
    "        'bin_date': 'day4-7',\n",
    "        'modelID': 'standard',\n",
    "    },\n",
    "     'params_init_dict': {\n",
    "            'beta':   0.5, \n",
    "            'bias_l': 0.0,\n",
    "            'zeta':  0.75,\n",
    "            'alpha': 0.25,\n",
    "     }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "529d0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### batch_run stuff\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "dir_save = '/path/to/save/results'\n",
    "dir_save = Path(dir_save)\n",
    "\n",
    "\n",
    "## standard libraries\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## get date\n",
    "dateToday = datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "from Q_learning_2ABT import qLearning_models as qLearning, helpers\n",
    "\n",
    "# unpack descriptors and hyperparameters\n",
    "geno = params['descriptors']['geno']\n",
    "bin_date = params['descriptors']['bin_date']\n",
    "modelID = params['descriptors']['modelID']\n",
    "lr = params['hyper_params']['lr']\n",
    "n_iter = params['hyper_params']['n_iter']\n",
    "seed = params['hyper_params']['seed']\n",
    "train_prop = params['hyper_params']['train_prop']\n",
    "split = params['hyper_params']['split']\n",
    "\n",
    "# set global seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# put all the info in saved file names\n",
    "saveInfoString = ('_' + geno +'_Fit_' + modelID + '_lr' + str(lr) + '_iter' + str(n_iter) + '_seed' + str(seed) + '_train_' + str(train_prop) + '_split_' + str(split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "597a43dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "data = pd.read_csv(os.path.join(params['paths']['dir_data'], params['paths']['filename']),index_col=False)\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "## split each session into split = params['hyper_params']['split'] consecutive parts and label them uniquely\n",
    "data1 = data.groupby('Session')\n",
    "tmp = data1.apply(lambda x: np.array_split(x.index,split))\n",
    "data['SessionSplit'] = np.nan\n",
    "for i in tmp.index:\n",
    "    for j in range(split):\n",
    "        data.loc[tmp[i][j],'SessionSplit'] = i + '_' + str(j)\n",
    "\n",
    "# save actual sessions as TrueSession, switch SessionSplit into Session:\n",
    "data['TrueSession'] = data['Session']\n",
    "data['Session'] = data['SessionSplit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0cda49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMR255\n",
      "before\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [22:10<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day4-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [22:11<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMR268\n",
      "before\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [21:00<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day4-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [41:12<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMR269\n",
      "before\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [18:20<00:00,  9.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day4-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [2:22:21<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMR274\n",
      "before\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [21:56<00:00,  7.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day4-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [23:44<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMR277\n",
      "before\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [19:25<00:00,  8.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day4-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [26:21<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMR276\n",
      "before\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [23:55<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day4-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [23:27<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMR279\n",
      "before\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [18:03<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day4-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [22:59<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMR287\n",
      "before\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [15:32<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day4-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [16:44<00:00,  9.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMR288\n",
      "before\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [20:28<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day4-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [20:56<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMR284\n",
      "before\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [20:28<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day4-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [24:25<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMR286\n",
      "before\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [17:24<00:00,  9.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day4-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [21:52<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMR292\n",
      "before\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [18:03<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day4-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [21:26<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMR298\n",
      "before\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [22:56<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day4-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10000/10000 [25:38<00:00,  6.50it/s]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# initialize results / logger dictionaries.\n",
    "results = {'mouse': [], 'epoch': [], 'beta': [], 'bias_l': [], 'zeta': [], 'alpha': [], 'nll': [], 'n_iteration': [], 'accuracy': [],'greedyAcc':[], 'n': [],\n",
    "          'nll_test': [], 'accuracy_test': [], 'greedyAcc_test': [], 'n_test': []}\n",
    "\n",
    "masterLogger = pd.DataFrame({'action_l': [], 'prob_l': [],'reward': [], 'Q_l': [], 'Q_r': [], 'session': [], 'mouse': [], 'epoch': []})\n",
    "masterTestLogger = pd.DataFrame({'action_l': [], 'prob_l': [],'reward': [], 'Q_l': [], 'Q_r': [], 'session': [], 'mouse': [], 'epoch': []})\n",
    "\n",
    "for mouse in data['Mouse'].unique():\n",
    "    print(mouse)\n",
    "    for epoch in ['before', params['descriptors']['bin_date']]:        \n",
    "        print(epoch)\n",
    "    \n",
    "        data_mouse_epoch = data.loc[(data['Mouse'] == mouse) & (data['binDate']==epoch)]\n",
    "\n",
    "        truSesh = data_mouse_epoch.TrueSession.unique()\n",
    "        stratifyClass = np.repeat(truSesh,np.repeat(split,len(truSesh))) # stratify parameter of train_test_split assigns approx. same proportion of each class to train and test.\n",
    "\n",
    "        # randomly select train and test sessions for each condition, and produce training dataframe\n",
    "        if train_prop < 1:\n",
    "            if split > 3: #enforce stratifying sessions into 4 or more\n",
    "                train_session_ids, test_session_ids = train_test_split(data_mouse_epoch.Session.unique(), train_size=train_prop, shuffle = True, random_state=seed, stratify=stratifyClass)\n",
    "            else:\n",
    "                train_session_ids, test_session_ids = train_test_split(data_mouse_epoch.Session.unique(), train_size=train_prop, shuffle = True, random_state=seed)\n",
    "            test = True\n",
    "        else:\n",
    "            train_session_ids = data_mouse_epoch['Session'].unique()\n",
    "            test_session_ids = np.empty(0, dtype=object)\n",
    "            test = False\n",
    "        \n",
    "        data_analyze = data_mouse_epoch.loc[data_mouse_epoch['Session'].isin(train_session_ids)]\n",
    "\n",
    "        \n",
    "        # create list of dictionaries containing Decision and Reward for each session\n",
    "        data_sessions = [{'Decision': data_analyze[data_analyze['Session']==u]['Decision'].to_numpy(),\n",
    "                          'Reward': data_analyze[data_analyze['Session']==u]['Reward'].to_numpy(),\n",
    "                         'blockPosition': data_analyze[data_analyze['Session']==u]['blockTrial'].to_numpy(),\n",
    "                         'Target': data_analyze[data_analyze['Session']==u]['Target'].to_numpy(),\n",
    "                         'DAB_I_HighProbSel': data_analyze[data_analyze['Session']==u]['DAB_I_HighProbSel'].to_numpy(),\n",
    "                         'Switch': data_analyze[data_analyze['Session']==u]['Switch'].to_numpy(),\n",
    "                         'DAB_I_flipLR_event': data_analyze[data_analyze['Session']==u]['DAB_I_flipLR_event'].to_numpy(),\n",
    "                         } for u in data_analyze['Session'].unique()]\n",
    "\n",
    "        # separate out lists of decisions and rewards\n",
    "        decisions_emp = list([torch.as_tensor(d['Decision'], dtype=torch.float32) for d in data_sessions])\n",
    "        rewards_emp = list([torch.as_tensor(d['Reward'], dtype=torch.float32) for d in data_sessions])\n",
    "        blockPositions = list([torch.as_tensor(d['blockPosition'], dtype=torch.float32) for d in data_sessions])\n",
    "        target = list([torch.as_tensor(d['Target'], dtype=torch.float32) for d in data_sessions])\n",
    "        DAB_I_HighProbSel = list([torch.as_tensor(d['DAB_I_HighProbSel'], dtype=torch.float32) for d in data_sessions])\n",
    "        switch = list([torch.as_tensor(d['Switch'], dtype=torch.float32) for d in data_sessions])\n",
    "        DAB_I_flipLR_event = list([torch.as_tensor(d['DAB_I_flipLR_event'], dtype=torch.float32) for d in data_sessions])                                            \n",
    "\n",
    "        \n",
    "        modelParams = torch.as_tensor([t for t in params['params_init_dict'].values()])\n",
    "        modelParams.requires_grad_(True)\n",
    "\n",
    "        optimizer = torch.optim.SGD(params=[modelParams], lr=params['hyper_params']['lr'])\n",
    "        fn_loss = torch.nn.NLLLoss()\n",
    "\n",
    "        # convergence_checker = qLearning.Convergence_checker(tol_convergence=thresh, window_convergence=100)\n",
    "\n",
    "        loss_rolling = []\n",
    "\n",
    "        #Note: modelID must take values: 'standard', 'reduced'\n",
    "        for i_epoch in tqdm(range(params['hyper_params']['n_iter'])):\n",
    "            logger, loss_rolling = qLearning.epoch_step_batch(optimizer, fn_loss, loss_rolling, modelParams, decisions_emp, rewards_emp, blockPositions,\n",
    "                                                              target, DAB_I_HighProbSel, switch, DAB_I_flipLR_event, modelID)\n",
    "            # diff_window_convergence, loss_smooth, converged = convergence_checker(loss_rolling)\n",
    "#             if converged:\n",
    "#                 print('converged')\n",
    "#                 break   \n",
    "#             if i_epoch == (n_epoch-1):\n",
    "#                 print('did not converge')\n",
    "#                 break\n",
    "                \n",
    "        \n",
    "        # convert logger into a df to be saved at the end.\n",
    "        mouseLogger = pd.concat([helpers.logger_to_df(helpers.append_dict(d, 'session', [ii]*len(d['prob_l']))) for ii,d in enumerate(logger)], axis=0)\n",
    "        mouseLogger['mouse'] = mouse\n",
    "        mouseLogger['epoch'] = epoch\n",
    "        masterLogger = pd.concat([masterLogger, mouseLogger], axis=0)\n",
    "        \n",
    "        params_detached = np.array([p.detach().cpu() for p in modelParams])\n",
    "        param_dict = dict(zip(params['params_init_dict'].keys(), np.array(params_detached)))\n",
    "\n",
    "        \n",
    "        #find policy accuracy\n",
    "        probs = np.stack([1-mouseLogger['prob_l'].to_numpy(), mouseLogger['prob_l'].to_numpy()], axis = 1) #2 column matrix of prob_l and 1-prob_l\n",
    "        decisions = np.concatenate([d.numpy().astype(int) for d in decisions_emp]) #convert decisions_emp list of tensors into one vector of ints.\n",
    "        decisions_oneHot = helpers.idx_to_oneHot(decisions)  #convert decisions to one-hot\n",
    "\n",
    "        confMatrix = helpers.confusion_matrix(probs, decisions_oneHot)\n",
    "        numDecisions = np.sum(decisions_oneHot, axis=0)\n",
    "        weightedAccuracy = (confMatrix.diagonal() * numDecisions).sum()/numDecisions.sum()\n",
    "        \n",
    "        #greedy policy:\n",
    "        probs_greedy = (probs > 0.5).astype(int)\n",
    "        confMatrix_greedy = helpers.confusion_matrix(probs_greedy, decisions_oneHot)        \n",
    "        weightedAccuracy_greedy = (confMatrix_greedy.diagonal() * numDecisions).sum()/numDecisions.sum()\n",
    "\n",
    "\n",
    "        results['beta'].append(param_dict['beta'])\n",
    "        results['bias_l'].append(param_dict['bias_l'])\n",
    "        results['zeta'].append(param_dict['zeta'])\n",
    "        results['alpha'].append(param_dict['alpha'])\n",
    "        results['mouse'].append(mouse)\n",
    "        results['epoch'].append(epoch)\n",
    "        results['nll'].append(loss_rolling[-1])\n",
    "        results['n_iteration'].append(i_epoch)\n",
    "        results['accuracy'].append(weightedAccuracy)\n",
    "        results['greedyAcc'].append(weightedAccuracy_greedy)\n",
    "        results['n'].append(np.sum([len(d) for d in decisions_emp]))\n",
    "        \n",
    "        \n",
    "       ########################################################### \n",
    "        # Now fit to Test data\n",
    "        if test:\n",
    "            data_analyze = data_mouse_epoch.loc[data_mouse_epoch['Session'].isin(test_session_ids)]\n",
    "\n",
    "            # create list of dictionaries containing Decision and Reward for each session\n",
    "            data_sessions = [{'Decision': data_analyze[data_analyze['Session']==u]['Decision'].to_numpy(),\n",
    "                          'Reward': data_analyze[data_analyze['Session']==u]['Reward'].to_numpy(),\n",
    "                         'blockPosition': data_analyze[data_analyze['Session']==u]['blockTrial'].to_numpy(),\n",
    "                         'Target': data_analyze[data_analyze['Session']==u]['Target'].to_numpy(),\n",
    "                         'DAB_I_HighProbSel': data_analyze[data_analyze['Session']==u]['DAB_I_HighProbSel'].to_numpy(),\n",
    "                         'Switch': data_analyze[data_analyze['Session']==u]['Switch'].to_numpy(),\n",
    "                         'DAB_I_flipLR_event': data_analyze[data_analyze['Session']==u]['DAB_I_flipLR_event'].to_numpy(),\n",
    "                         } for u in data_analyze['Session'].unique()]\n",
    "\n",
    "            testLogger = [qLearning.run_session(params=torch.as_tensor(params_detached), \n",
    "                                           mode_generative=False, \n",
    "                                           decisions_emp = torch.as_tensor(d['Decision'], dtype=torch.float32),\n",
    "                                           rewards_emp = torch.as_tensor(d['Reward'], dtype=torch.float32),\n",
    "                                            blockPosition = torch.as_tensor(d['blockPosition'], dtype=torch.float32),\n",
    "                                            target = torch.as_tensor(d['Target'], dtype=torch.float32),\n",
    "                                            DAB_I_HighProbSel = torch.as_tensor(d['DAB_I_HighProbSel'], dtype=torch.float32),\n",
    "                                            switch = torch.as_tensor(d['Switch'], dtype=torch.float32),\n",
    "                                            DAB_I_flipLR_event = torch.as_tensor(d['DAB_I_flipLR_event'], dtype=torch.float32),                                            \n",
    "                                            modelID = modelID,\n",
    "                                           ) for d in data_sessions]\n",
    "\n",
    "            mouseTestLogger = pd.concat([helpers.logger_to_df(helpers.append_dict(d, 'session', [ii]*len(d['prob_l']))) for ii,d in enumerate(testLogger)], axis=0)\n",
    "            mouseTestLogger['mouse'] = mouse\n",
    "            mouseTestLogger['epoch'] = epoch\n",
    "            masterTestLogger = pd.concat([masterTestLogger, mouseTestLogger], axis=0)\n",
    "\n",
    "            #find policy accuracy\n",
    "            probs = np.stack([1-mouseTestLogger['prob_l'].to_numpy(), mouseTestLogger['prob_l'].to_numpy()], axis = 1) #2 column matrix of prob_l and 1-prob_l\n",
    "\n",
    "            decisions_emp = list([torch.as_tensor(d['Decision'], dtype=torch.float32) for d in data_sessions])# separate out lists of decisions and rewards\n",
    "            decisions = np.concatenate([d.numpy().astype(int) for d in decisions_emp]) #convert decisions_emp list of tensors into one vector of ints.\n",
    "            decisions_oneHot = helpers.idx_to_oneHot(decisions)  #convert decisions to one-hot\n",
    "\n",
    "            confMatrix = helpers.confusion_matrix(probs, decisions_oneHot)\n",
    "            numDecisions = np.sum(decisions_oneHot, axis=0)\n",
    "            weightedAccuracy = (confMatrix.diagonal() * numDecisions).sum()/numDecisions.sum()\n",
    "\n",
    "            #greedy policy:\n",
    "            probs_greedy = (probs > 0.5).astype(int)\n",
    "            confMatrix_greedy = helpers.confusion_matrix(probs_greedy, decisions_oneHot)        \n",
    "            weightedAccuracy_greedy = (confMatrix_greedy.diagonal() * numDecisions).sum()/numDecisions.sum()\n",
    "\n",
    "            #nll\n",
    "            nll = -sum(np.log(probs)[decisions_oneHot])/len(decisions)\n",
    "\n",
    "            results['nll_test'].append(nll)\n",
    "            results['accuracy_test'].append(weightedAccuracy)\n",
    "            results['greedyAcc_test'].append(weightedAccuracy_greedy)\n",
    "            results['n_test'].append(len(decisions))\n",
    "\n",
    "if not test:        \n",
    "    del results['nll_test']\n",
    "    del results['accuracy_test']\n",
    "    del results['greedyAcc_test']\n",
    "    del results['n_test']\n",
    "    \n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "results.to_csv(str(Path(dir_save) / (dateToday + saveInfoString + '_batch.csv')), index = False)  \n",
    "masterLogger.to_csv(str(Path(dir_save) / (dateToday + saveInfoString + '_batch_logger_train.csv')), index = False)\n",
    "masterTestLogger.to_csv(str(Path(dir_save) / (dateToday + saveInfoString + '_batch_logger_test.csv')), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94bf310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
